{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf2aZtfird-R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAbJj6qKvqdA",
        "outputId": "6653f8db-6a43-40ce-d3fb-f8e07504a1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IgppAsUcTdR8"
      },
      "outputs": [],
      "source": [
        "import os, torch, torchvision, tarfile, itertools, matplotlib, json\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder, CIFAR100\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_PATH = '/content/drive/Othercomputers/My Mac/School/Robotics'"
      ],
      "metadata": {
        "id": "Yb4lnuP2ETzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLXFKtSArgnB"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3CgXwHpVXM9",
        "outputId": "5bebadab-6c77-4bbe-ade4-069c6c894861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz to ./cifar100.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169168619/169168619 [00:04<00:00, 38460062.45it/s]\n"
          ]
        }
      ],
      "source": [
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "# Extract dataset\n",
        "with tarfile.open('./cifar100.tgz', 'r:gz') as tar:\n",
        "   tar.extractall(path='./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PLmhMiGZXQex"
      },
      "outputs": [],
      "source": [
        "# Data transforms (normalization & data augmentation)\n",
        "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "                         tt.RandomHorizontalFlip(),\n",
        "                         tt.ToTensor(),\n",
        "                         tt.Normalize(*stats,inplace=True)])\n",
        "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
        "test_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ukPQDZEXTXE",
        "outputId": "dfed5b9b-6329-4b07-f573-43bacecb5f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 40649739.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# PyTorch datasets\n",
        "train_ds = CIFAR100(root = 'data/', download = True, train = True, transform = train_tfms)\n",
        "valid_ds = CIFAR100(root = 'data/', download = True, train = True, transform = valid_tfms)\n",
        "test_ds  = CIFAR100(root = 'data/', train=False, transform = test_tfms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0fDdxoTXu6qY"
      },
      "outputs": [],
      "source": [
        "# Train and validation splits (samplers)\n",
        "num_train = len(train_ds)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(0.2 * num_train))\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i7K03fsEsxVp"
      },
      "outputs": [],
      "source": [
        "# PyTorch data loaders\n",
        "def prepare_dataloader(train_ds, valid_ds, batch_size, device):\n",
        "  train_dl = DataLoader(train_ds, batch_size, num_workers=2, pin_memory=True, sampler=train_sampler)\n",
        "  valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=2, pin_memory=True, sampler=valid_sampler)\n",
        "\n",
        "  train_dl = DeviceDataLoader(train_dl, device)\n",
        "  valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "\n",
        "  return train_dl, valid_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqoigGb_rmUQ"
      },
      "source": [
        "## Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8by24M1EXw9P"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6ofKT-BSZIXB"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TDdXBF8bZX3N"
      },
      "outputs": [],
      "source": [
        "# call model eval before doing any evaluation - good practice\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDc3kYArt7d"
      },
      "source": [
        "## Model (ResNet9) preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1MWXXdGtZS36"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        # 3 x 32 x 32\n",
        "        self.conv1 = conv_block(in_channels, 64)         # 64 x 32 x 32\n",
        "        self.conv2 = conv_block(64, 128, pool=True)      # 128 x 16 x 16\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128),\n",
        "                                  conv_block(128, 128))  # 128 x 16 x 16\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)    # 256 x 8 x 8\n",
        "        self.conv4 = conv_block(256, 512, pool=True)    # 512 x 4 x 4\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512),\n",
        "                                  conv_block(512, 512))  # 512 x 4 x 4\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4), # 512 x 1 x 1\n",
        "                                        nn.Flatten(),     # 512\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes)) # 100\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out1 = self.conv1(xb)\n",
        "        out2 = self.conv2(out1)\n",
        "        out3 = self.res1(out2) + out2\n",
        "        out4 = self.conv3(out3)\n",
        "        out5 = self.conv4(out4)\n",
        "        out6 = self.res2(out5) + out5\n",
        "        out = self.classifier(out6)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0j1nzmGryBU"
      },
      "source": [
        "## Hyperparameter selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjveax8sZ_P",
        "outputId": "74659c5d-cb1f-463a-e1f7-5eb04cd3709f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Use CUDA (GPU) if available\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAic8f1Svyuv",
        "outputId": "6f5ea976-eba7-4e38-a88e-882d28ffd8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters recommended in original paper\n",
        "HYPERPARAMETERS = {\n",
        "    'epochs': [50, 100, 150],\n",
        "    'weight_decay': [1e-4],\n",
        "    'max_learning_rate': [0.005, 0.01, 0.02],\n",
        "    'gradient_clip': [0.1],\n",
        "    'batch_size': [256, 384, 512],\n",
        "}\n",
        "\n",
        "# Get all possible combination of hyperparameter sets (for grid search tuning)\n",
        "keys, values = zip(*HYPERPARAMETERS.items())\n",
        "HYPERPARAMETERS_COMB = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "print(len(HYPERPARAMETERS_COMB)) # length = 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "phMRL24qBusN"
      },
      "outputs": [],
      "source": [
        "hyperparams_scores = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LgLW3R2VZmoT"
      },
      "outputs": [],
      "source": [
        "# Do hyperparameter tuning and save logs to file\n",
        "with open(f'{LOG_PATH}/hyperparameter_selection.txt', 'a+') as f_log:\n",
        "\n",
        "  # Go through all possible sets of hyperparameters\n",
        "  for hyperparams in HYPERPARAMETERS_COMB:\n",
        "\n",
        "    # Set hyperparameters\n",
        "    epochs, weight_decay, max_lr, grad_clip, batch_size = hyperparams['epochs'], \\\n",
        "      hyperparams['weight_decay'], hyperparams['max_learning_rate'], hyperparams['gradient_clip'], hyperparams['batch_size']\n",
        "\n",
        "    # Get training and validation data splits\n",
        "    train_dl, valid_dl = prepare_dataloader(train_ds, valid_ds, batch_size=batch_size, device=device)\n",
        "\n",
        "    # Train model and evaluate\n",
        "    f_log.write(f'Epochs: {epochs}, Max LR: {max_lr}, Grad Clip: {grad_clip}, Weight Decay: {weight_decay}, Optimizer: Adam.\\n')\n",
        "    model   = to_device(ResNet9(3, 100), device)\n",
        "    history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n",
        "                            grad_clip=grad_clip,\n",
        "                            weight_decay=weight_decay,\n",
        "                            opt_func=torch.optim.Adam)\n",
        "\n",
        "    # Reduce history size\n",
        "    for epoch in range(len(history)):\n",
        "      history[epoch]['lrs'] = history[epoch]['lrs'][::25]\n",
        "\n",
        "    # Save history to log file\n",
        "    f_log.write(json.dumps(history, indent=2, separators=(',', ': ')).replace('],', '],\\n'))\n",
        "    f_log.write('\\n\\n\\n')\n",
        "\n",
        "    # Save score in dict\n",
        "    hyperparams_scores[f'{epochs}_{max_lr}_{grad_clip}_{weight_decay}'] = history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fjVZQIvr0mo"
      },
      "source": [
        "## Final Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "veFZUEuvr2fM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88694a9d-9c43-46c2-cb78-41a17974c4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], last_lr: 0.00025, train_loss: 3.8838, val_loss: 3.2375, val_acc: 0.2187\n",
            "Epoch [1], last_lr: 0.00041, train_loss: 3.1186, val_loss: 2.7381, val_acc: 0.3070\n",
            "Epoch [2], last_lr: 0.00066, train_loss: 2.6670, val_loss: 2.5488, val_acc: 0.3564\n",
            "Epoch [3], last_lr: 0.00099, train_loss: 2.3721, val_loss: 2.3741, val_acc: 0.3867\n",
            "Epoch [4], last_lr: 0.00140, train_loss: 2.1548, val_loss: 2.0940, val_acc: 0.4450\n",
            "Epoch [5], last_lr: 0.00186, train_loss: 1.9884, val_loss: 2.0853, val_acc: 0.4555\n",
            "Epoch [6], last_lr: 0.00235, train_loss: 1.8283, val_loss: 2.0739, val_acc: 0.4729\n",
            "Epoch [7], last_lr: 0.00285, train_loss: 1.6965, val_loss: 1.8550, val_acc: 0.5026\n",
            "Epoch [8], last_lr: 0.00334, train_loss: 1.5720, val_loss: 1.8497, val_acc: 0.4990\n",
            "Epoch [9], last_lr: 0.00380, train_loss: 1.4751, val_loss: 1.6644, val_acc: 0.5393\n",
            "Epoch [10], last_lr: 0.00421, train_loss: 1.3964, val_loss: 1.7697, val_acc: 0.5295\n",
            "Epoch [11], last_lr: 0.00454, train_loss: 1.3438, val_loss: 1.6846, val_acc: 0.5417\n",
            "Epoch [12], last_lr: 0.00479, train_loss: 1.3040, val_loss: 1.7040, val_acc: 0.5352\n",
            "Epoch [13], last_lr: 0.00495, train_loss: 1.2665, val_loss: 1.7318, val_acc: 0.5426\n",
            "Epoch [14], last_lr: 0.00500, train_loss: 1.2314, val_loss: 1.6405, val_acc: 0.5628\n",
            "Epoch [15], last_lr: 0.00499, train_loss: 1.2056, val_loss: 1.6208, val_acc: 0.5660\n",
            "Epoch [16], last_lr: 0.00496, train_loss: 1.1882, val_loss: 1.5613, val_acc: 0.5794\n",
            "Epoch [17], last_lr: 0.00491, train_loss: 1.1629, val_loss: 1.6827, val_acc: 0.5512\n",
            "Epoch [18], last_lr: 0.00484, train_loss: 1.1520, val_loss: 1.5447, val_acc: 0.5824\n",
            "Epoch [19], last_lr: 0.00475, train_loss: 1.1330, val_loss: 1.5967, val_acc: 0.5894\n",
            "Epoch [20], last_lr: 0.00465, train_loss: 1.1155, val_loss: 1.5350, val_acc: 0.5970\n",
            "Epoch [21], last_lr: 0.00452, train_loss: 1.0957, val_loss: 1.7496, val_acc: 0.5449\n",
            "Epoch [22], last_lr: 0.00438, train_loss: 1.0786, val_loss: 1.5308, val_acc: 0.5981\n",
            "Epoch [23], last_lr: 0.00423, train_loss: 1.0493, val_loss: 1.5211, val_acc: 0.5999\n",
            "Epoch [24], last_lr: 0.00406, train_loss: 1.0151, val_loss: 1.5949, val_acc: 0.5985\n",
            "Epoch [25], last_lr: 0.00388, train_loss: 0.9916, val_loss: 1.3510, val_acc: 0.6303\n",
            "Epoch [26], last_lr: 0.00368, train_loss: 0.9672, val_loss: 1.3265, val_acc: 0.6353\n",
            "Epoch [27], last_lr: 0.00348, train_loss: 0.9340, val_loss: 1.3506, val_acc: 0.6398\n",
            "Epoch [28], last_lr: 0.00327, train_loss: 0.8945, val_loss: 1.3171, val_acc: 0.6465\n",
            "Epoch [29], last_lr: 0.00306, train_loss: 0.8506, val_loss: 1.3969, val_acc: 0.6364\n",
            "Epoch [30], last_lr: 0.00284, train_loss: 0.8116, val_loss: 1.3345, val_acc: 0.6442\n",
            "Epoch [31], last_lr: 0.00261, train_loss: 0.7658, val_loss: 1.3209, val_acc: 0.6496\n",
            "Epoch [32], last_lr: 0.00239, train_loss: 0.7163, val_loss: 1.2041, val_acc: 0.6807\n",
            "Epoch [33], last_lr: 0.00216, train_loss: 0.6624, val_loss: 1.2692, val_acc: 0.6771\n",
            "Epoch [34], last_lr: 0.00194, train_loss: 0.5982, val_loss: 1.1947, val_acc: 0.6876\n",
            "Epoch [35], last_lr: 0.00173, train_loss: 0.5451, val_loss: 1.2516, val_acc: 0.6838\n",
            "Epoch [36], last_lr: 0.00152, train_loss: 0.4984, val_loss: 1.1686, val_acc: 0.6953\n",
            "Epoch [37], last_lr: 0.00132, train_loss: 0.4227, val_loss: 1.2507, val_acc: 0.7004\n",
            "Epoch [38], last_lr: 0.00112, train_loss: 0.3605, val_loss: 1.1726, val_acc: 0.7070\n",
            "Epoch [39], last_lr: 0.00094, train_loss: 0.3175, val_loss: 1.1918, val_acc: 0.7115\n",
            "Epoch [40], last_lr: 0.00077, train_loss: 0.2570, val_loss: 1.1669, val_acc: 0.7212\n",
            "Epoch [41], last_lr: 0.00062, train_loss: 0.2097, val_loss: 1.1794, val_acc: 0.7296\n",
            "Epoch [42], last_lr: 0.00048, train_loss: 0.1717, val_loss: 1.1765, val_acc: 0.7315\n",
            "Epoch [43], last_lr: 0.00035, train_loss: 0.1406, val_loss: 1.1690, val_acc: 0.7384\n",
            "Epoch [44], last_lr: 0.00025, train_loss: 0.1125, val_loss: 1.1660, val_acc: 0.7411\n",
            "Epoch [45], last_lr: 0.00016, train_loss: 0.0932, val_loss: 1.1899, val_acc: 0.7399\n",
            "Epoch [46], last_lr: 0.00009, train_loss: 0.0853, val_loss: 1.1867, val_acc: 0.7440\n",
            "Epoch [47], last_lr: 0.00004, train_loss: 0.0766, val_loss: 1.1814, val_acc: 0.7441\n",
            "Epoch [48], last_lr: 0.00001, train_loss: 0.0704, val_loss: 1.1840, val_acc: 0.7447\n",
            "Epoch [49], last_lr: 0.00000, train_loss: 0.0703, val_loss: 1.1869, val_acc: 0.7446\n"
          ]
        }
      ],
      "source": [
        "# Best hyperparameters from above\n",
        "epochs, weight_decay, max_lr, grad_clip, batch_size = 50, 1e-4, 0.005, 0.1, 256\n",
        "\n",
        "# Prepare entire training dataset, i.e., combined train and validation data\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "\n",
        "# Get official testing data\n",
        "test_dl  = DataLoader(test_ds, batch_size*2, pin_memory=True)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "\n",
        "# Train the model on optimal hyperparameters\n",
        "model   = to_device(ResNet9(3, 100), device)\n",
        "history = fit_one_cycle(epochs, max_lr, model, train_dl, test_dl,\n",
        "                        grad_clip=grad_clip,\n",
        "                        weight_decay=weight_decay,\n",
        "                        opt_func=torch.optim.Adam)\n",
        "\n",
        "# Save final training and testing logs\n",
        "with open(f'{LOG_PATH}/hyperparameter_selection_final.txt', 'w') as f_log:\n",
        "  f_log.write(json.dumps(history, indent=2, separators=(',', ': ')).replace('],', '],\\n'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Jf2aZtfird-R",
        "BLXFKtSArgnB",
        "mqoigGb_rmUQ",
        "hPDc3kYArt7d",
        "c0j1nzmGryBU",
        "2fjVZQIvr0mo"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}